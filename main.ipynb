{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a Basic Sentiment Analysis Project | Aadi Kulkarni\n",
    "# Libraries Used\n",
    "Tensorflow/Keras - Used for defining, building, and training the model\n",
    "Pandas - Used for loading the dataset\n",
    "Numpy - Used for working with arrays\n",
    "Matplotlib - Used for plotting the data\n",
    "Scikit-learn - Used for splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in our dataset\n",
    "This will take out CSV filed named `data.csv` and create a dataframe out of it with **Pandas**.\n",
    "X is going to be the input data. It is getting the column named `text` from the dataframe.\n",
    "y is going to be the output data. It is getting the column named `label` from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv') # Dataframe\n",
    "\n",
    "X = df['text'] # Input data\n",
    "y = df['label'] # Output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "This section of code is going to do the following:\n",
    "- Tokenize the text (Eg. \"Hello, my name is John\" -> ['Hello', 'my', 'name', 'is', 'John'])\n",
    "- Convert the text to sequences (Eg. ['Hello', 'my', 'name', 'is', 'John'] -> [[3, 6, 8, 2, 4]])\n",
    "- pad the sequences so they are all the same dimension (Eg. [[3, 6, 8, 2, 4], [3, 6, 8, 2, 4]] -> [[0, 0, 0, 0, 0, 0, 3, 6, 8, 2, 4], [0, 0, 0, 0, 0, 0, 3, 6, 8, 2, 4]]) This is because the model expects all the sequences to be the same dimension or same length so each sequence has a use, there are no \"holes\" in the sequence.\n",
    "\n",
    "# Important Note\n",
    "We aren't preprocessing the labels/y/output data because unlike X, the y is already in a numerical format that all has the same dimension. \"1 and 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert the text to sequences\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# pad the sequences so they are all the same dimension\n",
    "X_pad = pad_sequences(X_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now \"visualize\" what the X data might look like the the neural network. We can do this by printing out the first 5 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of X_pad: [[   0    0    0 ...  280  243    8]\n",
      " [   0    0    0 ...   47  543   93]\n",
      " [   0    0    0 ...  162   38  496]\n",
      " [   0    0    0 ...    1 1114  455]\n",
      " [   0    0    0 ...    3  510  652]]\n",
      "Preview of X_seq:  [[10, 219, 927, 11, 216, 119, 14, 110, 2, 655, 7567, 2383, 80, 1153, 4243, 13, 619, 8, 9, 3, 2593, 18, 93, 27, 257, 2, 1365, 14878, 3026, 95, 2, 435, 14879, 418, 1023, 10, 1775, 12, 1, 2299, 13, 29, 1, 92, 20, 1, 76, 502, 4, 1, 636, 820, 143, 10, 96, 25, 39, 2015, 53, 3, 2697, 637, 1, 636, 5, 79, 1, 2299, 38, 55, 5, 1, 220, 5, 430, 1, 1169, 18, 93, 132, 21, 39, 79, 53, 3, 1192, 637, 1, 815, 5, 104, 220, 8, 149, 1248, 3551, 5, 12, 10, 311, 34, 10, 948, 5, 39, 3176, 3027, 20, 1, 5845, 3, 180, 902, 1983, 65, 8, 1124, 16, 67, 47, 1983, 65, 8, 1124, 13, 32, 350, 4, 11, 118, 32, 350, 44, 60, 10, 345, 50, 121, 582, 12, 10, 65, 283, 173, 89, 2, 50, 3938, 863, 7040, 18361, 4834, 464, 20, 1240, 4395, 38, 24641, 2127, 15, 153, 1617, 5, 145, 54, 4396, 53, 15, 3, 283, 173, 10, 13, 321, 199, 11, 118, 6, 2, 990, 12667, 4598, 4243, 45, 29, 1, 1660, 3177, 4, 2, 8928, 14880, 24642, 1, 382, 24, 3178, 1449, 18, 100, 14881, 6, 5, 12668, 97, 144, 552, 36, 1, 525, 46, 6, 405, 58, 272, 5, 601, 644, 36, 684, 193, 5, 1, 693, 220, 1366, 15, 2, 68, 4, 24643, 3, 2, 18362, 3, 6580, 168, 983, 4, 11, 11135, 8929, 145, 5, 641, 7, 7, 582, 7, 7, 582, 16, 1, 352, 806, 14882, 24644, 4, 1826, 1036, 5063, 5846, 848, 806, 106, 291, 11, 235, 285, 9851, 565, 8, 1161, 1423, 3, 28, 6, 1881, 82, 49, 6, 28, 64, 70, 1200, 36, 110, 159, 22, 288, 6, 5, 98, 4243, 165, 49, 211, 37, 273, 5, 98, 3803, 9852, 165, 49, 18, 5063, 2384, 9, 119, 15, 385, 276, 140, 8178, 2, 18363, 3552, 8, 806, 18364, 34, 28, 68, 1073, 9, 168, 277, 197, 1826, 1036, 11, 118, 57, 5847, 4, 2, 1144, 18365, 261, 8179, 11136, 1193, 24645, 4243, 123, 1, 411, 15, 2, 24646, 4082, 138, 28, 11137, 3939, 22, 3679, 3, 285, 15, 1, 9853, 20, 1, 1719, 4, 22, 2698, 452, 394, 16, 1826, 1036, 5063, 14883, 1660, 3940, 80, 28, 8930, 1, 5550, 4, 280, 243, 8], [2889, 290, 1565, 704, 3553, 331, 41, 4, 162, 7568, 961, 14884, 782, 1436, 2128, 8931, 2485, 15, 2, 167, 52, 11138, 9, 284, 5, 67, 37, 1, 539, 24, 387, 176, 1483, 4, 11, 480, 3, 9, 88, 26, 205, 47, 543, 5, 94, 43, 33, 122, 1201, 5, 3680, 53, 3, 141, 46, 179, 710, 8, 11, 350, 4, 61, 2633, 33, 24, 24647, 8, 4244, 16, 2, 24648, 4599, 35, 1291, 94, 199, 85, 1, 24649, 5, 864, 47, 656, 247, 6, 29, 44, 10, 471, 1450, 1, 782, 14885, 15, 939, 46, 6, 153, 73, 319, 582, 1, 4244, 2437, 346, 44, 14886, 92, 85, 1, 17, 51, 42, 408, 1934, 132, 33, 24, 387, 4245, 93, 886, 2330, 93, 4245, 173, 93, 886, 2330, 173, 11, 6, 14887, 89, 30, 83, 35, 86, 388, 1, 4600, 187, 3, 9, 259, 8, 61, 2533, 4, 9, 287, 1, 144, 187, 88, 26, 61, 349, 1108, 43, 46, 6, 27, 55, 54, 391, 155, 320, 5, 505, 16, 11, 355, 24650, 8, 1, 98, 249, 975, 66, 33, 245, 63, 25, 2, 412, 4, 147, 2385, 61, 3453, 1506, 4835, 66, 47, 543, 93], [11, 17, 13, 34, 837, 416, 3, 518, 10, 1367, 2129, 1037, 218, 139, 1, 17, 1, 810, 8, 1, 17, 24, 2634, 3, 55, 150, 1, 107, 6, 205, 31, 48, 2090, 9, 6, 100, 212, 4246, 3, 31, 48, 760, 645, 43, 23, 24, 1720, 38, 917, 23, 88, 7041, 1, 500, 8, 1, 17, 3, 26, 752, 18, 43, 23, 24, 162, 38, 1170, 11, 6, 2, 19, 12, 88, 1827, 128, 678, 3, 26, 2817, 15, 29, 1, 3554, 60, 10, 227, 783, 24, 837, 229, 3, 691, 23, 69, 48, 7569, 5, 1, 944, 8, 11, 17, 308, 50, 49, 3, 1, 204, 13, 50, 1222, 80, 9, 13, 37, 33, 69, 273, 5, 1661, 53, 1, 1125, 18, 9, 155, 1260, 31, 29, 20, 2, 2220, 4, 328, 162, 162, 110, 322, 328, 110, 475, 1, 111, 8, 11, 17, 6, 2, 420, 6581, 602, 6, 1024, 8, 203, 18, 15, 11, 241, 4, 17, 8, 48, 4, 1, 52, 633, 135, 40, 111, 13, 1109, 51, 59, 89, 48, 4, 40, 976, 5848, 10, 694, 1038, 41, 1424, 10, 101, 1, 248, 151, 44, 11, 17, 6, 417, 1, 880, 16, 443, 1, 171, 120, 40, 2130, 516, 41, 5, 26, 1, 402, 1, 437, 949, 6, 24651, 82, 40, 558, 516, 438, 3, 437, 10, 101, 12, 13, 950, 413, 3, 82, 24652, 6581, 602, 5849, 69, 29, 40, 5849, 31, 369, 110, 3346, 30, 5288, 12, 13, 176, 645, 5, 34, 5, 24653, 9, 29, 2, 1222, 17, 18, 842, 43, 337, 162, 38, 496], [1, 90, 205, 151, 44, 14888, 881, 7042, 6, 1, 152, 18366, 77, 12669, 14, 873, 8180, 9854, 1, 278, 106, 6, 2, 270, 15, 2, 200, 509, 620, 722, 45, 1008, 8, 2, 1365, 1527, 3, 35, 856, 14889, 5, 444, 8, 14888, 60, 13, 22, 347, 493, 15, 40, 200, 509, 18367, 406, 6, 50, 24654, 59, 6, 24655, 14890, 4397, 1955, 3, 634, 1984, 3, 8181, 15, 9, 3, 2486, 2763, 30, 516, 3, 42, 29, 8, 40, 406, 1, 19, 87, 394, 602, 14891, 4836, 14, 2950, 2, 129, 35, 1003, 40, 51, 40, 521, 2179, 193, 557, 85, 14888, 35, 543, 5, 490, 2, 5551, 8, 493, 3, 35, 984, 40, 186, 29, 1, 56, 6200, 477, 40, 1594, 547, 4, 585, 8, 22, 9855, 602, 6, 1, 1154, 340, 8, 8932, 199, 143, 6582, 16, 22, 166, 15, 1299, 1855, 9856, 3, 18368, 18369, 4836, 2221, 16, 442, 4398, 1882, 4, 544, 3, 1, 2386, 3, 237, 130, 28, 285, 2, 12670, 106, 209, 2, 1401, 129, 18, 4, 265, 28, 160, 84, 205, 3, 7043, 2331, 28, 6, 1, 3681, 24656, 5, 390, 18367, 106, 36, 5064, 246, 7, 7, 873, 8180, 9854, 6, 2, 2818, 1797, 59, 301, 5, 1, 169, 493, 60, 6, 2, 6583, 266, 2, 241, 4, 329, 1544, 14889, 2, 493, 39, 37, 1749, 319, 14, 2950, 553, 39, 14, 28, 6, 8, 2, 92, 39, 37, 250, 319, 40, 121, 397, 6, 14892, 14, 121, 939, 24, 18, 6584, 6201, 3, 12671, 31, 210, 100, 22, 334, 341, 5, 14893, 3, 51, 28, 3804, 84, 28, 1566, 5552, 4601, 5, 14893, 28, 274, 5, 369, 3, 8180, 9854, 1003, 811, 3, 1402, 3, 24657, 46, 6, 2, 295, 4, 266, 8, 1, 19, 55, 150, 1, 266, 6, 8, 2, 295, 1749, 83, 1113, 8, 1, 693, 7044, 3, 303, 732, 280, 3, 8180, 24658, 14889, 3805, 6, 1211, 2050, 6, 113, 64, 24659, 130, 246, 36, 1, 191, 484, 3, 91, 8182, 8180, 9854, 190, 21, 5, 1025, 1, 3179, 59, 6, 8, 7, 7, 133, 372, 543, 3, 8180, 9854, 156, 3454, 849, 15, 9, 8, 1, 115, 597, 92, 18, 9, 543, 3, 59, 189, 406, 1, 2819, 18, 59, 182, 59, 274, 5, 1368, 2, 9857, 6, 1012, 18, 211, 58, 12672, 796, 59, 453, 34, 14894, 59, 24660, 3, 274, 5, 2, 1721, 173, 1484, 2387, 32, 9858, 45, 75, 14895, 40, 5, 7045, 59, 596, 2635, 3, 733, 15, 11, 18, 51, 59, 1045, 21, 63, 5, 2951, 1, 9857, 18, 5, 141, 5, 1, 1261, 5, 399, 84, 34, 12, 568, 6, 376, 4, 18370, 3, 9, 4602, 40, 29, 123, 173, 59, 453, 12673, 3, 1984, 3, 59, 58, 1110, 596, 12674, 8, 2300, 3, 9, 221, 409, 72, 12, 7, 7, 18366, 77, 12669, 410, 40, 29, 8, 11, 536, 6202, 3, 24661, 214, 873, 8933, 24662, 195, 26, 2, 50, 49, 164, 43, 32, 291, 4, 1, 9859, 4, 602, 14891, 4836, 11139, 3267, 8183, 16, 84, 12, 6, 1009, 2180, 5, 1300, 18371, 4, 1202, 8934, 4399, 81, 267, 108, 1353, 2181, 4083, 24663, 1856, 3108, 3, 24664, 3553, 25, 4603, 84, 41, 14, 27, 4, 1, 918, 2699, 4, 22, 9860, 1114, 1249, 8184, 18, 11, 6, 21, 14, 1134, 2, 19, 14, 154, 4, 76, 3028, 827, 620, 166, 198, 109, 134, 14, 18372, 4247, 12675, 18368, 18369, 4836, 3, 1, 18373, 209, 18374, 4400, 1299, 1855, 9856, 9, 195, 945, 940, 14, 18371, 553, 14, 2, 241, 4, 18375, 1985, 156, 444, 130, 1292, 3, 93, 6585, 3, 197, 2091, 516, 85, 133, 4, 2, 660, 3, 48, 56, 95, 12, 2, 730, 24665, 1857, 8, 392, 1956, 18, 12, 5850, 21, 63, 190, 1618, 3, 24666, 1, 19, 14896, 3, 2131, 91, 6586, 1858, 1, 124, 3, 93, 326, 18376, 41, 15, 58, 295, 4, 32, 300, 46, 24, 87, 4248, 8, 1, 201, 8180, 9854, 306, 5851, 4837, 15, 40, 509, 3, 160, 74, 2534, 29, 364, 43, 59, 6, 8185, 16, 2222, 37, 3555, 8, 1545, 8186, 24667, 3555, 1301, 60, 227, 1339, 40, 6587, 3, 3941, 2534, 12, 215, 133, 12, 6, 2764, 1546, 11, 6, 32, 205, 19, 448, 2, 1194, 27, 18, 27, 12, 856, 27, 8935, 3, 18377, 95, 1485, 27, 139, 32, 985, 24668, 7, 7, 32, 3029, 6588, 4, 1, 169, 753, 19, 1486, 1320, 31, 6589, 1859, 24669, 1321, 12, 45, 229, 199, 30, 3028, 977, 8, 1, 1114, 455], [51, 10, 81, 358, 44, 5289, 238, 18378, 10, 155, 548, 73, 18, 10, 192, 15, 1, 199, 83, 1, 199, 1883, 1, 199, 204, 3, 1860, 23, 96, 31, 224, 98, 2, 6590, 17, 44, 1, 3109, 5289, 303, 190, 5, 26, 767, 44, 18, 24670, 1126, 42, 34, 4084, 24671, 42, 2, 655, 24672, 4, 47, 9, 6, 157, 20, 8, 18379, 34, 373, 126, 4, 265, 42, 21, 29, 44, 24673, 18, 23, 140, 548, 52, 36, 2, 17, 211, 110, 3455, 14, 1, 3556, 5289, 17, 7, 7, 3, 1046, 36, 29, 1, 2594, 480, 9, 64, 6, 2, 74, 17, 1507, 111, 3, 2, 252, 355, 107, 4838, 48, 4, 1, 152, 25, 2016, 8, 76, 99, 12, 33, 24, 158, 177, 1024, 34, 9, 64, 189, 26, 329, 880, 1884, 7, 7, 86, 1212, 142, 5289, 238, 18378, 910, 23, 24, 2952, 217, 36, 48, 445, 493, 8, 986, 2595, 3, 174, 5, 832, 5, 1, 191, 484, 95, 23, 1828, 369, 93, 23, 227, 158, 161, 9, 705, 3, 510, 652]]\n",
      "Shape of X_seq 1 and X_seq 2: 387 197\n",
      "Shape of X_pad 1 and X_pad 2: 2156 2156\n"
     ]
    }
   ],
   "source": [
    "print(\"Preview of X_pad:\", X_pad[:5])\n",
    "print('Preview of X_seq: ', X_seq[:5])\n",
    "# This is to show why we pad the sequences\n",
    "print(\"Shape of X_seq 1 and X_seq 2:\", len(X_seq[0]), len(X_seq[1]))\n",
    "print(\"Shape of X_pad 1 and X_pad 2:\", len(X_pad[0]), len(X_pad[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split our data into train and test sets. This is so that we can train our model on the data, we can evaluate it's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "We will now be defining the model with the following architecture and layers.\n",
    "In a nutshell, here is the job of each layer:\n",
    "- **Embedding Layer:** This layer takes in an integer matrix of size (input_dim, output_dim) as input and produces an output matrix of size (input_dim, output_dim) as output. This layer is used to learn word vectors.\n",
    "- **GlobalAveragePooling1D:** This layer takes in a list of vectors and returns a vector with the average of the list of vectors.\n",
    "- **Dropout:** This layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "- **Dense:** This layer has 32 units which are used to compute an output and uses the relu activation function.\n",
    "- **Dense:** This layer has 16 units which are used to compute an output and uses the relu activation function.\n",
    "- **Dense:** This layer has 1 unit which is used to compute an output and uses the sigmoid activation function to output a value between 0 and 1 or the probability of the input being true/positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
